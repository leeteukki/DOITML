{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한꺼번에 다운받는 데 필요한 처리 내용\n",
    "\n",
    "지금까지 BeautifulSoup와 CSS 선택자의 사용법을 살펴봤음. 하지만 이것만으로는 링크에 있는 것을 한꺼번에 다운받을 수 없다.\n",
    "\n",
    "일단 a 태그의 링크 대상이 상대 경로일 수 있음. 그래서 링크 대상이 HTML일 경우, 해당 HTML의 내영에 추가적인 처리를 해야함. 그리고 링크를 재귀적으로 다운받아야함.\n",
    "\n",
    "이번 절에서는 링크에 있는 것을 한꺼번에 다운받는 기법을 소개하겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 상대 경로를 전개하는 방법\n",
    "\n",
    "그림 일단 첫 번째 문제부터 살펴보자.\n",
    "\n",
    "일단 a 태그의 href 속성에 링크대상이 \".../img/hoge.png\"처럼 상대 경로로 적혀 있다고 하자.\n",
    "a 태그가 상대 경로로 주어졌을 때 대상에 있는 것을 다운받으려면 상대 경로를 절대 경로로 변환해야함.\n",
    "\n",
    "상대 경로를 전개할 때는 urllib.parse.urljoin()을 사용해야함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/html/b.html\n",
      "http://example.com/html/sub/c.html\n",
      "http://example.com/index.html\n",
      "http://example.com/img/hogo.png\n",
      "http://example.com/css/hogo.css\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print( urljoin(base, \"b.html\"))\n",
    "print( urljoin(base, \"sub/c.html\"))\n",
    "print( urljoin(base, \"../index.html\"))\n",
    "print( urljoin(base, \"../img/hogo.png\"))\n",
    "print( urljoin(base, \"../css/hogo.css\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 기본 URL 기반으로 상대 경로를 절대 경로로 변환한다는 것을 알 수 있음. \n",
    "이처럼 상대 경로를 절대 경로로 변환하는 urljoin() 함수의 사용법을 살펴보자\n",
    "\n",
    "urllib.parse.urljoin() 사용법\n",
    "urljoin(bast, path)\n",
    "\n",
    "이 함수는 첫 번째 매개변수로 기본 URL, 두 번째 매개변수로 상대 경로를 지정함.\n",
    "\n",
    "만약 상대 경로(path 매개변수)가 http:// 등으로 시작한다면 기본 URL(bsse 매개변수)을 무시하고 두 번쨰 매개변수에 지정한 URL을 리턴함. \n",
    "\n",
    "예제코드를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/html/hogo.html\n",
      "http://otherExample.com/wiki\n",
      "http://anotherExample.org/test\n"
     ]
    }
   ],
   "source": [
    "print( urljoin(base, \"hogo.html\"))\n",
    "print( urljoin(base, \"http://otherExample.com/wiki\"))\n",
    "print( urljoin(base, \"//anotherExample.org/test\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 urljoin() 함수를 사용하면 a 태그의 href 속성에 지정돼 있는 경로를 절대 경로로 쉽게 변환가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 페이지를 한꺼번에 다운받는 프로그램\n",
    "\n",
    "그럼 이를 구현한 프로그램을 살펴보자. 일단 이번 절에서는 웹에 있는 파이썬 문서 중에서 library 폴더 아래에 있는 모든 것을 다운받아 보겠음.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램\n",
    "# 모듈 읽어 들이기 - (1)\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from os import makedirs\n",
    "import os.path, time, re\n",
    "\n",
    "# 이미 처리한 파일인지 확인하기 위한 변수 - (2)\n",
    "proc_files = {}\n",
    "\n",
    "# HTML 내부에 있는 링크를 추출하는 함수 - (3)\n",
    "def enum_links(html, base) :\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel='stylesheet']\") # CSS\n",
    "    links += soup.select(\"a[href]\") # 링크\n",
    "    result = []\n",
    "    \n",
    "    # href 속성을 추출하고, 링크를 절대 경로로 변환 - (4)\n",
    "    for a in links:\n",
    "        href = a.ttrs['href']\n",
    "        url = urljoin(base, href)\n",
    "        result.append(url)\n",
    "        return result\n",
    "    \n",
    "    # 파일을 다운받고 저장하는 함수 - (5)\n",
    "    def download_file(url):\n",
    "        o = urlparse(url)\n",
    "        savepath = \"./\" + o.netloc + o.path\n",
    "        if re.search(r\"/$\", savepath): # 폴더라면 index.html\n",
    "            savepath += \"index.html\"\n",
    "        savedir = os.path.dirname(savepath)\n",
    "        \n",
    "        # 모두 다운됐는지 확인\n",
    "        if os.path.exists(savepath): return savepath\n",
    "        \n",
    "        # 다운받을 폴더 생성\n",
    "        if not os.path.exists(savedir):\n",
    "            print(\"mkdir = \", savedir)\n",
    "            makedirs(savedir)\n",
    "            \n",
    "        # 파일 다운받기 - (6)\n",
    "        try :\n",
    "            print(\"download = \", url)\n",
    "            urlretrieve(url, savepath)\n",
    "            time.sleep(1) # 1초 휴식 - (7)\n",
    "            return savepath\n",
    "        except:\n",
    "            print(\"다운 실패:\", url)\n",
    "            return none\n",
    "        \n",
    "        # HTML을 분석하고 다운받는 함수 - (8)\n",
    "        def analyze_html(url, root_url):\n",
    "            savepath = download_file(url)\n",
    "            \n",
    "            if savepath in None : return\n",
    "            if savepath is proc_files: return # 이미 처리 했다면 처리하지 않음 - (9)\n",
    "            proc_files[savepath] = True\n",
    "            print(\"analyze_html\", url)\n",
    "            \n",
    "        # 링크 추출 - (10)\n",
    "        html = open(savepath, \"r\", encoding=\"utf-8\").read()\n",
    "        links = enum_links(html, url)\n",
    "        \n",
    "        for link_url in links:\n",
    "            #링크가 루트 이외의 경로를 나타내면 무시 - (11)\n",
    "            if link_url.find(root_url) != 0:\n",
    "                if not re.search(r\".css$\",link_url): continue\n",
    "            # HTML 이라면\n",
    "            if re.search(r\".(html|html)$\", link_url):\n",
    "                # 재귀적으로 HTML 파일 분석하기\n",
    "                analyze_html(link_url, root_url)\n",
    "                continue\n",
    "            # 기타파일\n",
    "            dwonload_file(link_url)\n",
    "            \n",
    "            # URL에 있는 모든 것 다운로드 받기\n",
    "            if __name__ == \"__name__\":\n",
    "                url = \"https://docs.python.org/3/\"\n",
    "                analyze_html(url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
