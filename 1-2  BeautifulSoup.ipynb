{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup로 스크레이핑하기\n",
    "\n",
    "스크레이핑이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것임. 최근에는 인터넷에 데이터가 너무 많으므로 스크레이핑을 잘 활용하는 것이 중요함.\n",
    "\n",
    "파이썬으로 스크레이핑할 때 뺴놓을 수 없는 라이브러리가 바로 \"BeautifulSoup\"임. 이 라이브러리를 이용하면 간단하게 HTML과 XML에서 정보를 추출할 수 있음.\n",
    "\n",
    "최근 스크레이핑 라이브러리는 다운로드부터 HTML 분석까지느 모두 해주는 경우가 많은데, BeautifulSoup는 어디까지나 HTML, XML을 분석해주는 라이브러리임. \n",
    "BeautifulSoup 자체에는 다운로드 기능이 없으므로 주의."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 설치\n",
    "\n",
    "파이썬 라이브러리를 설치할 때는 pip 명령어를 사용함. pip이란 파이썬 패키지 관리 시스템임. \n",
    "파이썬 패키지는 Python Package Index에서 확인할 수 있음. pip를 이용하면 PyPI에 있는 패키지를 명령어 한 줄로 설치할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.7/site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: beautifulsoup4 in /opt/anaconda3/lib/python3.7/site-packages (4.9.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>1.2 in /opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  BeautifulSoup 기본 사용법\n",
    "\n",
    "일단은 BeautifulSoup의 기본적인 사용법을 확인해보자.\n",
    "다음 프로그램은 BeautifulSoup를 이용해 분석하는 간단한 예제임. 웹사이트로부터 HTML을 가져와서 사용하는 것이 아니라 HMTL을 문자열로 만들어 사용하고 있음. 그리고 문자열 분석을 완료하면 결과를 출력함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =스크레이핑이란?\n",
      "p = 웹 페이지를 분석하는 것\n",
      "p = 원하는 부분을 추출하는 것<.P>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어들이기 - (1)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석하고 싶은 HTML -(2)\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1>스크레이핑이란?</h1>\n",
    "    <p>웹 페이지를 분석하는 것</p>\n",
    "    <p>원하는 부분을 추출하는 것<.P>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 - (3)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기 - (4)\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 요소의 글자 출력하기 - (5)\n",
    "print(\"h1 =\" + h1.string)\n",
    "print(\"p = \" + p1.string)\n",
    "print(\"p = \" + p2.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)에서는 BeautifulSoup 라이브러리를 읽어 들임.\n",
    "(2)에서는 분석 대상 HTML을 지정함.\n",
    "(3)에서는 BeautifulSoup 인스턴스를 생성함. 이때 첫 번째 매개변수에 HTML을 지정함. 그리고 두 번째 매개변수에는 분석할 분석기(parser)의 종류를 지정함.\n",
    "HTML을 분석할 때는 \"html.parser\"라고 지정함.\n",
    "\n",
    "(4)에서는 원하는 부분을 추출함. 정상적으로 분석됐다면 HTML의 구조처럼 루트 요소인 <html>에서 마침표(.)를 사용해 값에 접근할 수 있음. 코드에서는 \"soup.html.body.h1\"이라고 적었는데, <html><body>h1 에 있는 요소에 접근한 것임.\n",
    "(5)에서는 string 속성에 접근해서 요소의 글자 부분을 추출함.\n",
    "    \n",
    "HTML 내부에는 p 태그가 2개 있는데 soup.html.body.p 라고 접근하면 앞쪽에 있는 p 태그를 추출하게 됨. 이때 첫 번째의 next_sibling에서는 </p> 뒤에 있는 줄바꿈 또는 공백이 추출됨. 따라서 next_sibling을 한 번 더 사용해 2번째 p 태그를 추출함.\n",
    "    \n",
    "HTML의 구조를 알고 있다면 쉽게 원하는 요소를 추출할 수 있음. 하지만 루트부터 \"html.body..\"형태로 HTML 구조를 하나하나 적어 나가는 것은 조금 귀찮고 복잡함. 따라서 이어서 간단하게 요소를 찾아내는 방법을 소개하겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## id로 요소를 찾는 방법\n",
    "\n",
    "Beautifulsoup는 루트부터 하나하나 요소를 찾는 방법 말고도 id 속성을 지정해서 요소를 찾는 find()매서드라는 매서드를 제공함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크래이핑이란?\n",
      "#body=  원하는 부분을 추출하는 것 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1 id = \"title\">스크래이핑이란?</h1>\n",
    "    <p>웹 페이지를 분석하는 것 </p1>\n",
    "    <p id = \"body\"> 원하는 부분을 추출하는 것 </p>\n",
    "</html></body>\n",
    "\"\"\"\n",
    "\n",
    "# html 분석하기 - (1)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# find() 메서드로 원하는 부분 추출하기 - (2)\n",
    "title = soup.find(id=\"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "# 텍스트 부분 추력하기\n",
    "print(\"#title = \"  + title.string)\n",
    "print(\"#body= \" + body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)에서는 BeautifulSoup 인스턴스를 생성.\n",
    "첫 번째 매개변수에 분석하고 싶은 HTML을 지정함.\n",
    "\n",
    "(2)에서는 id를 지정해 요소를 추출함. find() 메서드에 \"id=<값>\" 형태로 매개변수를 지정해 요소를 검색함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 개의 요소 추출하기 - find_all() 메서드\n",
    "\n",
    "참고로 여러 개의 태그를 한 번에 추출하고 싶을 땨는 find_all() 메서드를 사용함. 다음 코드는 HTML 내부에 있는 여러 개의 a 태그를 추출하는 프로그램임.\n",
    "\n",
    "a 태그는 하이퍼링크 태그이므로, 대상은 href 속성으로 지정하고 링크를 설명하는 텍스트는 태그 내부에 입력함.\n",
    "\n",
    "다음 코드는 설명 글자와 링크 대상 URL을 추출하고 출력하는 예임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    " <body> \n",
    "  <ul>\n",
    "    <li><a href = \"http://www.naver.com\">naver</a></li>\n",
    "    \n",
    "    <li><a href = \"http://www.daum.net\">daum</a></li>\n",
    "  </ul>\n",
    " </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 - (1)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# find_all() 매서드로 추출하기 - (2)\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# 링크 목록 출력하기 - (3)\n",
    "for a in links :\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)에서는 HTML을 지정해 BeautifulSoup 인스턴스를 생성함.\n",
    "(2)에서는 find_all() 메서드를 사용해 a 태그를 추출함.\n",
    "\n",
    "프로그램의 (3)에서는 추출한 모든 요소를 for 구문으로 반복처리함. 링크의 href 속성은 attrs['href']처럼 attrs 속성에 추출함. 또한 내부의 설명 텍스트는 string 속성으로 추출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urlopen()과 BeautifulSoup 조합하기\n",
    "\n",
    "BeautifulSoup 인스턴스를 생성하는 방법을 배웠음. 지금까지 살펴본 예제처럼 HTML 문자열을 지정할 수도 있지만 open() 함수 또는 urllib.request.urlopen() 함수의 리턴 값을 지정해도됨.\n",
    "\n",
    "그럼 urlopen()을 사용해 \"기상철 RES\"에서 특정 내용을 추출해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 10월 1일(목)~2일(금)은 서울.경기도와 강원영서에 비가 오겠습니다.<br />○ (기온) 이번 예보기간 낮 기온은 19~25도로 오늘(20~27도)과 비슷하거나 조금 낮겠고, 아침 기온은 7~18도로 선선하겠습니다. <br />          특히, 내륙을 중심으로 낮과 밤의 기온차가 10도 내외로 크겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "# urlopen()으로 데이터 가져오기 - (1)\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기 - (2)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기 - (3)\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)에서는 url을 염. \n",
    "(2)에서는 url을 BeautifulSoup로 분석을 함.\n",
    "(3)에서는 html내에서 title 속성값을 추출하고 wf 값을 추출해서 print 문으로 추출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS 선택자를 사용하기 \n",
    "\n",
    "BeautifulSoup 는 자바스크립트 라이브러이인 jQuery처럼 css 선택자를 지정해서 원하는 요소를 추출하는 기능도 제공.\n",
    "그럼 이러한 메서드를 사용하는 예를 살펴보자. 다음은 HTML에서 h1 태그와 li 태그를 추출해서 출력하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =  위키북스 도서\n",
      "li =  유니티 게임 이펙트 입문\n",
      "li =  스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li =  모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석 대상 HTML - (1)\n",
    "html = \"\"\"\n",
    "<html> <body>\n",
    "<div id = \"meigen\">\n",
    "    <h1>위키북스 도서</h1>\n",
    "    <ul class = \"items\">\n",
    "        <li>유니티 게임 이펙트 입문</li>\n",
    "        <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "        <li>모던 웹사이트 디자인의 정석</li>\n",
    "    </ul>\n",
    "</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 - (2)\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# 타이틀 부분 추출하기 - (3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(\"h1 = \", h1)\n",
    "\n",
    "# 목록 부분 추출하기 - (4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list :\n",
    "    print(\"li = \", li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)에서는 분석대상 html을 지정함.\n",
    "(2)에서는 html을 BeautifulSoup를 분석함.\n",
    "(3)select_one 메서드를 이용해 지정한 h1 태그를 추출함. (실제로 웹 사이트에서 추출한 HTML에서 원하는 요소를 선택하려면 CSS 선택자를 조금 더 자세히 지정해야함)\n",
    "(4)에서는 select 메서드를 이용해 h1 태그를 추출함. 그리고 추출한 요소는 리스트 자료형이므로 for 구문을 사용해 하나씩 출력함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 금융에서 환율 정보 추출하기\n",
    "\n",
    "그럼 실제로 웹 사이트를 스크래핑 해보겠음.\n",
    "\n",
    "이번 절에서는 다양한 금융 정보가 공개돼 있는 \"네이버 금융\"에서 원/달러 환율 정보를 추출해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kr/usd =  1,175.00\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML 가져오기 \n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기 \n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"kr/usd = \", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
